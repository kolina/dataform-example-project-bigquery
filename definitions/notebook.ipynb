{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQwVnLRLgeQk"
   },
   "source": [
    "**V3: Claim Preprocessing**\n",
    "\n",
    "Input:\n",
    "-  A single BQ table with a column for Jefferson publication numbers.\n",
    "\n",
    "What happens:\n",
    "- This colab will handle all the data fetching, joining, parsing, that we previously did in 6 steps.\n",
    "\n",
    "It will generate three output tables.\n",
    "\n",
    "Output Table: a claims-level table with the following columns:\n",
    "\n",
    "\n",
    "     -  claim_publication_number (str)\n",
    "     -  claim_id (str)\n",
    "     -  claim_num_str (str)\n",
    "     -  claim_num_int (int)\n",
    "     -  claim_type (str)\n",
    "     -  claim_indep (TRUE/FALSE)\n",
    "     -  claim_indep_order (int)\n",
    "     -  claim_text (text)\n",
    "     -  claim_list_parentclaim_ids (str)\n",
    "     -  claim_list_parentclaim_nums (str)\n",
    "     -  claim_list_parentclaim_texts (str)\n",
    "     -  claim_list_children_ids (str)\n",
    "     -  claim_immediate_parent_id (str)\n",
    "     -  claim_immediate_child_id (str)\n",
    "\n",
    "\n",
    "Output:\n",
    "- Save answers into the following tables to BigQuery using original input table name with the extensions of:\n",
    "  - gpatents.publications_v2.us_gpt_claims\n",
    "\n",
    "\n",
    "More Info:\n",
    "- See this [doc](https://docs.google.com/document/d/1t7KpJkHVhbK3D6C3syMo0QYVicB92YjAW39ALGL4Rrk/edit?pli=1&tab=t.chh254ik1msx)\n",
    "\n",
    "Older Versions:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Init Inputs\n",
    "\n",
    "GPATENTS_PROJECT = \"gpatents\" # @param [\"gpatents\"]\n",
    "PATENTS_DST_PROJECT = \"google.com:patents-dst\" # @param [\"google.com:patents-dst\"]\n",
    "LOCATION = \"us-central1\" # @param [\"us-central1\"]\n",
    "\n",
    "GPATENTS_DATASET_ID = \"publications_v2\" # @param [\"publications_v2\"]\n",
    "PATENTS_DST_DATASET_ID = \"elysian\" # @param [\"elysian\"]\n",
    "\n",
    "INPUT_TABLE_ID = \"us_claims_backlog\" # @param [\"us_claims_backlog\"]\n",
    "\n",
    "####################\n",
    "\n",
    "OUTPUT_CLAIMS_TABLE_ID = INPUT_TABLE_ID.replace('_backlog', '')\n",
    "\n",
    "print(f\"INPUT_TABLE_ID: {INPUT_TABLE_ID}\")\n",
    "\n",
    "print(f\"OUTPUT_CLAIMS_TABLE_ID: {OUTPUT_CLAIMS_TABLE_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAU2vDNfglv-"
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installs\n",
    "!pip install -U spacy pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "from ast import literal_eval\n",
    "from time import sleep, time\n",
    "from datetime import date, datetime\n",
    "from pandas_gbq.schema.pandas_to_bigquery import dataframe_to_bigquery_fields\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pytz\n",
    "PST = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Set, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from google.colab import drive\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.colab import auth, runtime\n",
    "\n",
    "bq_client = bigquery.Client(project=GPATENTS_PROJECT)\n",
    "num_workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper Functions - Generic\n",
    "\n",
    "################################################################################\n",
    "# Generic helper functions\n",
    "def get_timestamp(timestamp_label:bool=False):\n",
    "  prefix = \"Timestamp:\" if timestamp_label else \"\"\n",
    "  return f\"\"\"{prefix} {datetime.now(PST).strftime(\"%Y/%m/%d %I:%M %p\")}\"\"\"\n",
    "\n",
    "\n",
    "# Get Colab runtime uptime\n",
    "\n",
    "def get_uptime(hours_needed:int=0):\n",
    "  \"\"\"Get the uptime of the Colab runtime.\"\"\"\n",
    "  # Execute the uptime command and get the result\n",
    "  uptime_result = subprocess.run(['uptime', '-p'], capture_output=True, text=True)\n",
    "\n",
    "  # Parse the output to get the uptime duration\n",
    "  uptime_str = uptime_result.stdout.strip()\n",
    "  if hours_needed > 0:\n",
    "    if \"hours\" in uptime_str:\n",
    "      hours_left = 24 - int(get_uptime().replace(\"System Uptime: up \",\"\").split(\"hours\")[0].strip())\n",
    "    else:\n",
    "      hours_left = 23\n",
    "    if hours_left < hours_needed:\n",
    "      return f\"!!!!! WARNING !!!!! System Uptime: {uptime_str} | !!!! {hours_left} hours left but {hours_needed} hours needed !!!!\"\n",
    "  return f\"System Uptime: {uptime_str}\"\n",
    "\n",
    "\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | vCPUs: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Generate Unique ID\n",
    "pacific_tz = pytz.timezone('US/Pacific')\n",
    "\n",
    "\n",
    "def generate_custom_unique_id(include_random=False, include_counter=False, counter_digits:int=3, include_uuid=False):\n",
    "  current_datetime = datetime.now(pacific_tz)\n",
    "\n",
    "  # Get components separately\n",
    "  year = current_datetime.strftime('%Y')\n",
    "\n",
    "  # Convert month abbreviation to uppercase\n",
    "  month_abbr = current_datetime.strftime('%b').upper()\n",
    "\n",
    "  day = current_datetime.strftime('%d')\n",
    "  hour = current_datetime.strftime('%H')\n",
    "  if hour > \"11\":\n",
    "    if hour > \"12\":\n",
    "      hour = str(int(hour) - 12)\n",
    "    am_pm = \"PM\"\n",
    "  else:\n",
    "    am_pm = \"AM\"\n",
    "  minute = current_datetime.strftime('%M')\n",
    "  second = current_datetime.strftime('%S')\n",
    "\n",
    "  # timestamp = f\"{day}_{month_abbr}_{year}_{hour}_{minute}_{second}\"\n",
    "  timestamp = f\"{day}{month_abbr}{year}_{hour}{minute}{am_pm}\"\n",
    "  parts = [timestamp]\n",
    "\n",
    "  if include_random:\n",
    "    random_number = random.randint(1000, 9999)\n",
    "    parts.append(f\"{random_number}\")\n",
    "\n",
    "  if include_counter:\n",
    "    global counter, last_timestamp\n",
    "    if 'counter' not in globals():\n",
    "      counter = 0\n",
    "      last_timestamp = timestamp\n",
    "    if timestamp != last_timestamp:\n",
    "      counter = 0\n",
    "      last_timestamp = timestamp\n",
    "    else:\n",
    "      counter += 1\n",
    "    parts.append(f\"{counter:0{counter_digits}d}\")\n",
    "\n",
    "  if include_uuid:\n",
    "    short_uuid = str(uuid.uuid4())[:8]\n",
    "    parts.append(short_uuid)\n",
    "\n",
    "  unique_id = '_'.join(parts)\n",
    "  return unique_id\n",
    "\n",
    "# Example usage\n",
    "unique_id = generate_custom_unique_id(include_counter=True)\n",
    "print(\"Generated Unique ID:\", unique_id)\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Big Query Helper Functions\n",
    "import uuid\n",
    "\n",
    "def does_table_exist(bq_client, table_id):\n",
    "  try:\n",
    "    bq_client.get_table(table_id)\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "def does_dataset_exist(bq_client, dataset_id):\n",
    "  try:\n",
    "    bq_client.get_dataset(dataset_id)\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "def df_to_bq(\n",
    "    bq_client: bigquery.Client,\n",
    "    df: pd.DataFrame,\n",
    "    merge_keys: List[str],\n",
    "    output_table: str,\n",
    "    output_dataset: str,\n",
    "    project: str,\n",
    "    output_table_description: str,\n",
    "    output_table_schema: List[bigquery.SchemaField],\n",
    "    job_config_labels: Dict[str, str],\n",
    "    batch_size: int = 50000,\n",
    "    debug_flag: bool = False\n",
    "):\n",
    "  dataset_exists = does_dataset_exist(bq_client, output_dataset)\n",
    "  table_exists = does_table_exist(bq_client, output_table)\n",
    "\n",
    "  if dataset_exists:\n",
    "    pass\n",
    "  else:\n",
    "    bq_client.create_dataset(output_dataset)\n",
    "\n",
    "  if table_exists and not merge_keys:\n",
    "    if debug_flag:\n",
    "      print(f\"Table '{output_table}' exists. Data will be appended.\")\n",
    "    write_disposition = \"WRITE_APPEND\"\n",
    "  elif table_exists and merge_keys:\n",
    "    if debug_flag:\n",
    "      print(f\"Table '{output_table}' exists. Data will be merged and inserted.\")\n",
    "    write_disposition = \"WRITE_MERGE\"\n",
    "  else:\n",
    "    if debug_flag:\n",
    "      print(f\"Table '{output_table}' does not exist.\")\n",
    "\n",
    "    write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "  if write_disposition == \"WRITE_MERGE\":\n",
    "    try:\n",
    "      table_id = f\"{project}.{output_table}\"\n",
    "      staging_table_id = f\"{table_id}_staging_{uuid.uuid4().hex}\"\n",
    "      print(f\"Inserting new data into Temp Table: {staging_table_id}\")\n",
    "\n",
    "      job_config = bigquery.LoadJobConfig(\n",
    "          schema=output_table_schema,\n",
    "          write_disposition=\"WRITE_TRUNCATE\", # Always truncate the temp table\n",
    "          labels = job_config_labels\n",
    "      )\n",
    "      load_job = bq_client.load_table_from_dataframe(\n",
    "          df, staging_table_id, job_config=job_config\n",
    "      )\n",
    "      load_job.result() # Wait for the staging load to complete\n",
    "\n",
    "      # 3. Construct and run the MERGE query\n",
    "      all_cols = [field.name for field in output_table_schema]\n",
    "      update_cols = [col for col in all_cols if col not in merge_keys]\n",
    "\n",
    "      on_clause = \" AND \".join([f\"T.`{key}` = S.`{key}`\" for key in merge_keys])\n",
    "      update_clause = \", \".join([f\"T.`{col}` = S.`{col}`\" for col in update_cols])\n",
    "      insert_clause_cols = \", \".join([f\"`{col}`\" for col in all_cols])\n",
    "      insert_clause_vals = \", \".join([f\"S.`{col}`\" for col in all_cols])\n",
    "\n",
    "      merge_sql = f\"\"\"\n",
    "        MERGE `{table_id}` AS T\n",
    "        USING (SELECT * FROM `{staging_table_id}` GROUP BY ALL) AS S\n",
    "        ON {on_clause}\n",
    "        WHEN MATCHED THEN\n",
    "          UPDATE SET {update_clause}\n",
    "        WHEN NOT MATCHED THEN\n",
    "          INSERT ({insert_clause_cols})\n",
    "          VALUES ({insert_clause_vals})\n",
    "      \"\"\"\n",
    "      # print(merge_sql)\n",
    "      merge_job = bq_client.query(merge_sql)\n",
    "      merge_job.result() # Wait for the MERGE to complete\n",
    "      print(f\"MERGE operation successful. {merge_job.num_dml_affected_rows} rows affected.\")\n",
    "    except Exception as e:\n",
    "      if \"UPDATE/MERGE must match at most one source row for each target row\" in str(e):\n",
    "        print(\"MERGE failed due to multiple matches. Deleting old records and retrying.\")\n",
    "\n",
    "        # 1. Get unique values of the merge_keys from the DataFrame\n",
    "        # Handle cases where merge_keys might not contain all columns\n",
    "        if all(key in df.columns for key in merge_keys):\n",
    "          unique_keys = df[merge_keys].drop_duplicates()\n",
    "        else:\n",
    "          print(\"Warning: One or more merge keys not found in DataFrame. Skipping DELETE and re-attempting MERGE.\")\n",
    "          raise e\n",
    "\n",
    "        # 2. Construct and run the DELETE statement\n",
    "        delete_statements = []\n",
    "        key_conditions = \"\"\n",
    "        for x in unique_keys.columns:\n",
    "          clause = ', '.join(f\"'{l}'\" for l in list(set(unique_keys[x])))\n",
    "          key_conditions += \" AND \".join(f\"{x} IN ({clause})\")\n",
    "        delete_statements.append(f\"DELETE FROM `{table_id}` WHERE {key_conditions};\")\n",
    "\n",
    "        delete_sql = \"\\n\".join(delete_statements)\n",
    "        print(f\"Executing DELETE statements:\\n{delete_sql}\")\n",
    "        delete_job = bq_client.query(delete_sql)\n",
    "        delete_job.result()\n",
    "\n",
    "        # 3. Re-run the MERGE query after deletion\n",
    "        print(\"Re-attempting MERGE after successful deletion.\")\n",
    "        merge_job = bq_client.query(merge_sql)\n",
    "        merge_job.result()\n",
    "        print(f\"Second MERGE operation successful. {merge_job.num_dml_affected_rows} rows affected.\")\n",
    "      else:\n",
    "        raise e # Re-raise any other exceptions\n",
    "    finally:\n",
    "      bq_client.delete_table(staging_table_id, not_found_ok=True)\n",
    "  else:\n",
    "    num_batches = (len(df) // batch_size) + (1 if len(df) % batch_size != 0 else 0)\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "      start_idx = batch_num * batch_size\n",
    "      end_idx = min((batch_num + 1) * batch_size, len(df))\n",
    "      batch_df = df.iloc[start_idx: end_idx]\n",
    "\n",
    "      job_config = bigquery.LoadJobConfig(\n",
    "          schema=output_table_schema,\n",
    "          write_disposition=write_disposition,\n",
    "          labels = job_config_labels\n",
    "      )\n",
    "      job = bq_client.load_table_from_dataframe(\n",
    "          batch_df, output_table, job_config=job_config\n",
    "      )\n",
    "      job.result()\n",
    "\n",
    "      if write_disposition == \"WRITE_TRUNCATE\":\n",
    "        write_disposition = \"WRITE_APPEND\"\n",
    "\n",
    "      sleep(1)\n",
    "\n",
    "  table = bq_client.get_table(output_table)\n",
    "  table.description = output_table_description\n",
    "  bq_client.update_table(table, [\"description\"])\n",
    "\n",
    "def save_output_to_bq(\n",
    "    df: pd.DataFrame,\n",
    "    input_tables_id: List[str],\n",
    "    output_table: str,\n",
    "    output_dataset: str,\n",
    "    project: str,\n",
    "    job_config_labels: Dict[str, str],\n",
    "    merge_keys: List[str] = None,\n",
    "    output_table_schema: List[bigquery.SchemaField] = None,\n",
    "    colab_description: str = \"Quick Description of it\",\n",
    "    table_notes: str = \"\",\n",
    "    display_timer: bool = False,\n",
    "    bq_client: bigquery.Client = None # pass BQ_CLIENT\n",
    "):\n",
    "  if bq_client is None:\n",
    "    raise ValueError(\"bq_client must be provided to save_output_to_bq\")\n",
    "  print(f\"{get_timestamp()} | Saving dataframe to BQ\")\n",
    "\n",
    "  input_tables_str = \" \\n\".join(input_tables_id)\n",
    "\n",
    "  output_table_description = f\"\"\"\n",
    "  Colab:\n",
    "    {colab_description}\n",
    "\n",
    "  Notes:\n",
    "    {table_notes}\n",
    "\n",
    "  Input Parameters:\n",
    "    INPUT_TABLES: {input_tables_str}\n",
    "  \"\"\"\n",
    "\n",
    "  if not output_table_schema:\n",
    "    output_table_schema = list(dataframe_to_bigquery_fields(df))\n",
    "\n",
    "  try:\n",
    "    df_to_bq(\n",
    "        bq_client=bq_client,\n",
    "        df=df,\n",
    "        output_table=output_table,\n",
    "        merge_keys=merge_keys,\n",
    "        project=project,\n",
    "        output_dataset=output_dataset,\n",
    "        output_table_description=output_table_description,\n",
    "        output_table_schema=output_table_schema,\n",
    "        job_config_labels=job_config_labels,\n",
    "        debug_flag=True\n",
    "    )\n",
    "    print(f\"{get_timestamp()} | Saved {len(df)} rows to {output_table}\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error saving data to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZvnPJxrP6lX"
   },
   "source": [
    "# Colab-specific Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Parsing Claims Helpers\n",
    "from copy import deepcopy\n",
    "from bs4 import NavigableString\n",
    "\n",
    "def parse_html_claims(row, debug=False):\n",
    "  soup = BeautifulSoup(row[\"claims_html\"], \"html.parser\")\n",
    "\n",
    "  claims = []\n",
    "\n",
    "  for claim_tag in soup.find_all(\"claim\"):\n",
    "    claim_id = claim_tag.get(\"id\")\n",
    "    claim_num = claim_tag.get(\"num\")\n",
    "    list_claim_tags = claim_tag.find_all(\"claim-text\")\n",
    "\n",
    "    try:\n",
    "      # We build a list of parts by iterating through tag contents\n",
    "      raw_parts = []\n",
    "      # NEW: A set to store the exact text of math formulas\n",
    "      math_formulas = set()\n",
    "\n",
    "      for i, tag in enumerate(list_claim_tags):\n",
    "        if len(list_claim_tags) > 1 and i == 0:\n",
    "          if list_claim_tags[1].get_text() in list_claim_tags[0].get_text():\n",
    "            first_limitation = list_claim_tags[0].get_text().split(list_claim_tags[1].get_text())[0]\n",
    "            raw_parts.extend(list(filter(lambda x: x != '', re.split('[:;]', first_limitation.replace(\"\\n\", \"\")))))\n",
    "          else:\n",
    "            math_tag = bool(tag.find(\"maths\"))\n",
    "            for part in re.split('[:;]', tag.get_text()):\n",
    "              if part and part.strip():\n",
    "                if math_tag:\n",
    "                  # If the content is a math tag, keep it whole\n",
    "                  # Extract the clean text from the formula\n",
    "                  formula_text = part.strip()\n",
    "                  raw_parts.append(formula_text)\n",
    "                  # NEW: Add the formula text to our set for later checking\n",
    "                  math_formulas.add(formula_text)\n",
    "\n",
    "                  # If it's a regular text string, split it\n",
    "                else:\n",
    "                  # Split only the text part by colon or semicolon\n",
    "                  text = part.strip().replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "                  if text not in ('and', 'where', '.', ' '):\n",
    "                    raw_parts.append(text)\n",
    "        else:\n",
    "          math_tag = bool(tag.find(\"maths\"))\n",
    "          for part in re.split('[:;]', tag.get_text()):\n",
    "            if part and part.strip():\n",
    "              if math_tag:\n",
    "                # If the content is a math tag, keep it whole\n",
    "                # Extract the clean text from the formula\n",
    "                formula_text = part.strip()\n",
    "                raw_parts.append(formula_text)\n",
    "                # NEW: Add the formula text to our set for later checking\n",
    "                math_formulas.add(formula_text)\n",
    "\n",
    "                # If it's a regular text string, split it\n",
    "              else:\n",
    "                # Split only the text part by colon or semicolon\n",
    "                text = part.strip().replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "                if text not in ('and', 'where', '.', ' '):\n",
    "                  raw_parts.append(text)\n",
    "\n",
    "      # Clean up the list by stripping whitespace and removing empty items\n",
    "      all_parts = [part.strip().replace(\"\\n\", \" \").replace(\"  \", \" \") for part in raw_parts if part and part.strip()]\n",
    "\n",
    "      # Merge singular connector words with the next element\n",
    "      merged_parts = []\n",
    "      i = 0\n",
    "      while i < len(all_parts):\n",
    "        current_part = all_parts[i]\n",
    "\n",
    "        # MODIFIED LINE: Check if the part is a single word AND NOT in our math_formulas set\n",
    "        if len(current_part.split()) == 1 and current_part not in math_formulas and i + 1 < len(all_parts):\n",
    "          # Join with the next part and append\n",
    "          next_part = all_parts[i+1]\n",
    "          merged_parts.append(f\"{current_part} {next_part}\")\n",
    "          # Skip the next element since we've already used it\n",
    "          i += 2\n",
    "        else:\n",
    "          # Otherwise, just add the current part as is\n",
    "          merged_parts.append(current_part)\n",
    "          i += 1\n",
    "\n",
    "      full_text = \" \".join(merged_parts)\n",
    "      limitations = \"#!#\".join(merged_parts)\n",
    "\n",
    "      parent_claim_ids = []\n",
    "      parent_claim_nums = []\n",
    "\n",
    "      parents = claim_tag.find_all(\"claim-ref\")\n",
    "\n",
    "      if parents:\n",
    "        for p in parents:\n",
    "          parent_claim_ids.append(p[\"idref\"])\n",
    "          parent_claim_nums.append(p[\"idref\"].split(\"CLM-\")[1])\n",
    "\n",
    "      claims.append({\n",
    "          \"publication_number\": row[\"publication_number\"],\n",
    "          \"publication_date\": row[\"publication_date\"],\n",
    "          \"claim_id\": claim_id,\n",
    "          \"claim_num_str\": claim_num,\n",
    "          \"claim_num_int\": int(claim_num),\n",
    "          \"claim_text\": full_text,\n",
    "          \"claim_type\": get_claim_type(full_text),\n",
    "          \"limitations\": limitations,\n",
    "          \"claim_list_parentclaim_ids\": parent_claim_ids,\n",
    "          \"claim_list_parentclaim_nums\": parent_claim_nums\n",
    "      })\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "  return pd.DataFrame(claims)\n",
    "\n",
    "def find_first_of_words(sentence, words):\n",
    "  pattern = rf'\\b(?:{\"|\".join(map(re.escape, words))})\\b'\n",
    "  matches = list(re.finditer(pattern, sentence, re.IGNORECASE))\n",
    "\n",
    "  if not matches:\n",
    "      return None\n",
    "\n",
    "  first_match = min(matches, key=lambda m: m.start())\n",
    "  return first_match.group()\n",
    "\n",
    "def get_claim_type(claim_text: str):\n",
    "  claim_intro = claim_text[:70].replace(\"-\", \" \").lower()\n",
    "  words = [\"method\", \"system\", \"apparatus\", \"medium\", \"device\",\n",
    "          \"cancelled\", \"canceled\", \"non transitory\", \"computer readable\"]\n",
    "  first_word = find_first_of_words(claim_intro, words)\n",
    "\n",
    "  if \"medium\" == first_word:\n",
    "    claim_type = \"CRM\"\n",
    "  elif \"system\" == first_word:\n",
    "    claim_type = \"System\"\n",
    "  elif \"method\" == first_word:\n",
    "    claim_type = \"Method\"\n",
    "  elif \"device\" == first_word:\n",
    "    claim_type = \"Apparatus\"\n",
    "  elif \"apparatus\" == first_word:\n",
    "    claim_type = \"Apparatus\"\n",
    "  elif \"canceled\" == first_word:\n",
    "    claim_type = \"Canceled\"\n",
    "  elif \"cancelled\" == first_word:\n",
    "    claim_type = \"Canceled\"\n",
    "  elif \"non transitory\" == first_word:\n",
    "    claim_type = \"CRM\"\n",
    "  elif \"computer readable\" == first_word:\n",
    "    claim_type = \"CRM\"\n",
    "  elif \"media\" == first_word:\n",
    "    claim_type = \"CRM\"\n",
    "  elif \"medium\" == first_word:\n",
    "    claim_type = \"CRM\"\n",
    "  else:\n",
    "    claim_type = \"Apparatus\"\n",
    "  return claim_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Patent Hierarchy & Generic Assessment\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def _clean_cell(cell):\n",
    "    \"\"\"\n",
    "    A robust helper function to replace NA-like values with an empty list.\n",
    "    It specifically avoids the 'truth value is ambiguous' error by checking\n",
    "    for array-like types before calling pd.isna().\n",
    "    \"\"\"\n",
    "    # If the cell is already a list, tuple, or numpy array, keep it as is.\n",
    "    if isinstance(cell, (list, tuple, np.ndarray)):\n",
    "        return cell\n",
    "    # For scalar values, check if it's NA/NaN/None and replace with an empty list.\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    # Otherwise, return the scalar value.\n",
    "    return cell\n",
    "\n",
    "def create_hierarchy_vectorized(df_junction, df_pub_claims):\n",
    "    \"\"\"\n",
    "    Vectorized replacement for the first slow loop using process_tree_hierarchy.\n",
    "    \"\"\"\n",
    "    if df_junction.empty:\n",
    "        return pd.DataFrame(columns=['publication_number', 'claim_id', 'claim_list_parentclaim_texts', 'claim_list_children_ids'])\n",
    "\n",
    "    # Create a lookup table: (publication_number, claim_id) -> claim_text\n",
    "    claim_text_lookup = df_pub_claims.groupby(\n",
    "        [\"publication_number\", \"claim_id\"]\n",
    "    )['claim_text'].first().reset_index()\n",
    "\n",
    "    # Rename columns to prepare for the merge\n",
    "    claim_text_lookup = claim_text_lookup.rename(columns={\n",
    "        'claim_id': 'claim_list_parentclaim_ids',\n",
    "        'claim_text': 'parent_text'\n",
    "    })\n",
    "\n",
    "    df_junction = pd.merge(\n",
    "        df_junction,\n",
    "        claim_text_lookup,\n",
    "        on=['publication_number', 'claim_list_parentclaim_ids'],\n",
    "        how='left' # 'left' join keeps all rows from the original dataframe\n",
    "    )\n",
    "\n",
    "    parent_texts_agg = df_junction.dropna(subset=['parent_text']) \\\n",
    "        .groupby(['publication_number', 'claim_id'])['parent_text'] \\\n",
    "        .agg(list) \\\n",
    "        .rename('claim_list_parentclaim_texts')\n",
    "\n",
    "    children_ids_agg = df_junction.groupby(['publication_number', 'claim_list_parentclaim_ids'])['claim_id'] \\\n",
    "        .agg(list) \\\n",
    "        .rename('claim_list_children_ids')\n",
    "    children_ids_agg.index.names = ['publication_number', 'claim_id']\n",
    "\n",
    "    df_hierarchy = pd.merge(\n",
    "        parent_texts_agg,\n",
    "        children_ids_agg,\n",
    "        on=['publication_number', 'claim_id'],\n",
    "        how='outer'\n",
    "    ).reset_index()\n",
    "\n",
    "    return df_hierarchy\n",
    "\n",
    "def find_nodes_vectorized(df_junction):\n",
    "    \"\"\"\n",
    "    Vectorized replacement for the second slow loop using find_intermediate_nodes_multi_parent_df.\n",
    "    \"\"\"\n",
    "    df_edges = df_junction.dropna(subset=['claim_list_parentclaim_ids'])\n",
    "\n",
    "    if df_edges.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    parent_ids_agg = df_edges.groupby(['publication_number', 'claim_id'])['claim_list_parentclaim_ids'] \\\n",
    "        .agg(list) \\\n",
    "        .rename('claim_immediate_parent_id')\n",
    "\n",
    "    child_ids_agg = df_edges.groupby(['publication_number', 'claim_list_parentclaim_ids'])['claim_id'] \\\n",
    "        .agg(list) \\\n",
    "        .rename('claim_immediate_child_id')\n",
    "    child_ids_agg.index.names = ['publication_number', 'claim_id']\n",
    "\n",
    "    df_nodes = pd.merge(\n",
    "        parent_ids_agg,\n",
    "        child_ids_agg,\n",
    "        on=['publication_number', 'claim_id'],\n",
    "        how='outer'\n",
    "    ).reset_index()\n",
    "\n",
    "    return df_nodes\n",
    "\n",
    "def generate_publication_hierarchy(df_pub_claims):\n",
    "  \"\"\"\n",
    "  Main function to process claim hierarchies, now using optimized helper functions.\n",
    "  \"\"\"\n",
    "  df_parent_child_junction = df_pub_claims[[\"publication_number\", \"claim_id\", \"claim_text\", \"claim_list_parentclaim_ids\"]].explode(\"claim_list_parentclaim_ids\")\n",
    "\n",
    "  df_pub_hierarchy = create_hierarchy_vectorized(df_parent_child_junction.copy(), df_pub_claims)\n",
    "\n",
    "  df_pub_claims_final = pd.merge(df_pub_claims, df_pub_hierarchy, how = \"left\", on = [\"publication_number\", \"claim_id\"])\n",
    "\n",
    "  df_intermediate_nodes = find_nodes_vectorized(df_parent_child_junction)\n",
    "\n",
    "  if df_intermediate_nodes.empty:\n",
    "    df_pub_claims_final[\"claim_immediate_parent_id\"] = [[] for _ in range(len(df_pub_claims_final))]\n",
    "    df_pub_claims_final[\"claim_immediate_child_id\"] = [[] for _ in range(len(df_pub_claims_final))]\n",
    "  else:\n",
    "    df_pub_claims_final = pd.merge(df_pub_claims_final, df_intermediate_nodes, how = \"left\", on = [\"publication_number\", \"claim_id\"])\n",
    "\n",
    "  # === FIX: Final cleanup using the robust helper function ===\n",
    "  list_cols = [\n",
    "      'claim_list_parentclaim_texts', 'claim_list_children_ids',\n",
    "      'claim_immediate_parent_id', 'claim_immediate_child_id'\n",
    "  ]\n",
    "  for col in list_cols:\n",
    "      if col in df_pub_claims_final.columns:\n",
    "          # Use the helper function to robustly fill NaNs with empty lists\n",
    "          df_pub_claims_final[col] = df_pub_claims_final[col].apply(_clean_cell)\n",
    "\n",
    "  return df_pub_claims_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Claim HTML Processor\n",
    "def process_pub_claim_html(df_input, debug=False):\n",
    "  list_columns = [\n",
    "      \"claim_list_parentclaim_ids\", \"claim_list_parentclaim_nums\",\n",
    "      \"claim_list_parentclaim_texts\", \"claim_list_children_ids\",\n",
    "      \"claim_immediate_parent_id\", \"claim_immediate_child_id\"\n",
    "  ]\n",
    "\n",
    "\n",
    "  df_pub_claims = pd.concat(df_input.apply(lambda x: parse_html_claims(x, debug), axis = 1).values, ignore_index=True)\n",
    "\n",
    "  df_pub_claims_final = generate_publication_hierarchy(df_pub_claims)\n",
    "\n",
    "  for x in list_columns:\n",
    "    df_pub_claims_final[x] = df_pub_claims_final[x].fillna(\"999\").apply(lambda x: \"||\".join(sorted(set(x))) if x != \"999\" else '')\n",
    "\n",
    "  # Step 1 & 2: Filter for 'indep'==True and sort by 'pub_number' and 'num_str'\n",
    "  df_pub_claims_final[\"claim_indep\"] = pd.isnull(df_pub_claims_final[\"claim_list_parentclaim_ids\"].replace(\"\", None))\n",
    "  filtered_sorted_df = df_pub_claims_final[df_pub_claims_final['claim_indep']].sort_values(by=['publication_number', 'claim_num_str'])\n",
    "\n",
    "  # Step 3: Group by 'pub_number' and apply cumcount()\n",
    "  filtered_sorted_df['claim_indep_order'] = filtered_sorted_df.groupby('publication_number').cumcount() + 1\n",
    "\n",
    "  # Step 4: Merge this series back to the original DataFrame\n",
    "  df_pub_claims_final = df_pub_claims_final.merge(filtered_sorted_df[['publication_number', 'claim_num_str', 'claim_indep_order']], on=['publication_number', 'claim_num_str'], how='left')\n",
    "\n",
    "  # Fill NaN values in 'claim_indep_order' with 0 and convert to integer\n",
    "  df_pub_claims_final['claim_indep_order'] = df_pub_claims_final['claim_indep_order'].fillna(0).astype(int)\n",
    "\n",
    "  df_pub_claims_final = df_pub_claims_final.drop(columns = [\"limitations\"])\n",
    "\n",
    "  return df_pub_claims_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMe8CUgChJVe"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Assert Data Checks\n",
    "def assert_data_checks(claims_data):\n",
    "  # No fields as NULL\n",
    "  null_cols = claims_data.isnull().sum()\n",
    "  null_cols = null_cols[null_cols > 0].index\n",
    "  assert int(claims_data.isnull().sum().sum()) == 0, f'Claims DataFrame has Null values, columns: {list(null_cols)}'\n",
    "\n",
    "  # Indep Order must be from 1 to Number of Independent Claims\n",
    "  assert (\n",
    "      pd.merge(\n",
    "        claims_data[claims_data[\"claim_indep\"]==True]\n",
    "        .groupby([\"publication_number\"]).size()\n",
    "        .to_frame()\n",
    "        .rename(columns={0:\"total_indep_claims\"}),\n",
    "        claims_data[claims_data[\"claim_indep\"]==True]\n",
    "        .groupby([\"publication_number\"]).agg({\"claim_indep_order\": lambda x: len(x)}),\n",
    "        how = \"left\",\n",
    "        right_index = True,\n",
    "        left_index = True\n",
    "    ).assign(\n",
    "        claim_indep_check = lambda x: x.total_indep_claims == x.claim_indep_order\n",
    "    ).query(\"claim_indep_check == False\").shape[0]\n",
    "  ) == 0, 'Independent Order of Claims do not match the number of independent Claims'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Step 1: Loading Data from Public Data\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | Loading Data from Public Data\")\n",
    "\n",
    "################################################################################\n",
    "\n",
    "label_colab = \"claim__preprocessing\" # Name of the colab; e.g. google_embeddings_vector_index\n",
    "label_step = \"loading_data_from_public_data\"  # Descriptive name of what is happening, e.g. backlog_query\n",
    "label_pipeline = \"claim_etl_v0\" # Name of the BigQuery pipeline; e.g. patent_embeddings_v0\n",
    "\n",
    "input_sql = f\"\"\"\n",
    "  WITH JeffersonTmp AS (\n",
    "    SELECT\n",
    "      JPubs.publication_number,\n",
    "      PARSE_DATE('%Y%m%d', CAST(JPubs.publication_date AS STRING)) AS publication_date,\n",
    "      HtmlClaims.text as html_claim_text,\n",
    "      GPubs.title AS title,\n",
    "      GPubs.abstract AS abstract,\n",
    "      GPubs.url AS gpatents_url\n",
    "    FROM `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{INPUT_TABLE_ID}` AS Pubs\n",
    "    INNER JOIN `jefferson-1790.patents.publications` AS JPubs\n",
    "    ON Pubs.publication_number = JPubs.publication_number\n",
    "    LEFT JOIN UNNEST(JPubs.claims_localized_html) AS HtmlClaims\n",
    "    LEFT JOIN `jefferson-1790.google_patents_research.publications` AS GPubs\n",
    "      ON GPubs.publication_number = JPubs.publication_number\n",
    "    GROUP BY\n",
    "      ALL\n",
    "  )\n",
    "\n",
    "  SELECT\n",
    "    JPubs.publication_number,\n",
    "    JPubs.publication_date,\n",
    "    JPubs.html_claim_text AS claims_html,\n",
    "    JPubs.title,\n",
    "    JPubs.abstract,\n",
    "    JPubs.gpatents_url\n",
    "  FROM JeffersonTmp AS JPubs\n",
    "  WHERE JPubs.publication_number IN (\n",
    "    SELECT\n",
    "      DISTINCT\n",
    "        publication_number\n",
    "    FROM `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}`\n",
    "  )\n",
    "  LIMIT 200000\n",
    "\"\"\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    labels = {\n",
    "        \"colab\": label_colab,\n",
    "        \"step\": label_step,\n",
    "        \"pipeline\": label_pipeline\n",
    "    }\n",
    ")\n",
    "\n",
    "query_result = bq_client.query(input_sql, job_config=job_config)\n",
    "\n",
    "df_input = query_result.result().to_dataframe()\n",
    "\n",
    "new_data_bool = df_input.shape[0]\n",
    "\n",
    "if not new_data_bool:\n",
    "  print(\"No new data found to process it. Task complete. Disconnecting runtime to save resources.\")\n",
    "  runtime.unassign()\n",
    "\n",
    "df_input_filt = df_input[pd.notnull(df_input[\"claims_html\"])].reset_index(drop=True)\n",
    "\n",
    "del df_input\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Step 2: Processing Claims in Parallel\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | Processing Limitations in Parallel\")\n",
    "\n",
    "################################################################################\n",
    "\n",
    "label_colab = \"claim_preprocessing\" # Name of the colab; e.g. google_embeddings_vector_index\n",
    "label_step = \"processing_claims_in_parallel\"  # Descriptive name of what is happening, e.g. backlog_query\n",
    "label_pipeline = \"claim_etl_v0\" # Name of the BigQuery pipeline; e.g. patent_embeddings_v0\n",
    "\n",
    "job_config_labels = {\n",
    "  \"colab\": label_colab,\n",
    "  \"step\": label_step,\n",
    "  \"pipeline\": label_pipeline\n",
    "}\n",
    "\n",
    "start_time = str(datetime.fromtimestamp(int(time()))).replace(' ', '_').replace(\":\", \"\").replace(\"-\", \"\")\n",
    "\n",
    "for y in tqdm_notebook(range(0, len(df_input_filt), 10000)):\n",
    "  df_input_re_filt = df_input_filt.iloc[y: y + 10000]\n",
    "\n",
    "  batch_size = \"5000\" # @param [10, 50, 100, 200, 500, 800, 1000, 1500, 2000, 5000, 10000] {allow_type: true}\n",
    "  batches = [df_input_re_filt.iloc[x: x + int(batch_size)] for x in range(0, len(df_input_re_filt), int(batch_size))]\n",
    "\n",
    "  del df_input_re_filt\n",
    "  gc.collect()\n",
    "\n",
    "  with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    # Submit the function for each batch to the executor\n",
    "    # This creates a list of 'Future' objects, which represent the running tasks\n",
    "    futures = [executor.submit(process_pub_claim_html, batch) for batch in batches]\n",
    "\n",
    "    # Use as_completed to process results as they finish\n",
    "    # Wrap it with tqdm for a live progress bar\n",
    "    for future in tqdm_notebook(as_completed(futures), total=len(batches), desc=\"Processing Batches\"):\n",
    "      try:\n",
    "        # You can get the result of the function call if it returns anything\n",
    "        claims_data = future.result()\n",
    "\n",
    "        assert_data_checks(claims_data)\n",
    "\n",
    "        claims_data[\"processed_date\"] = datetime.fromtimestamp(int(time())).date()\n",
    "        claims_data[\"processed_timestamp\"] = datetime.fromtimestamp(int(time()))\n",
    "\n",
    "        save_output_to_bq(\n",
    "          claims_data,\n",
    "          input_tables_id = [\n",
    "              f\"{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{INPUT_TABLE_ID}\"\n",
    "          ],\n",
    "          merge_keys = [\"publication_number\", \"claim_id\"],\n",
    "          output_table = f\"{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}_new_{start_time}\",\n",
    "          output_dataset = GPATENTS_DATASET_ID,\n",
    "          project = GPATENTS_PROJECT,\n",
    "          colab_description = \"Publications Claims Metrics\",\n",
    "          job_config_labels = job_config_labels,\n",
    "          bq_client = bq_client\n",
    "        )\n",
    "\n",
    "        del claims_data\n",
    "        gc.collect()\n",
    "      except Exception as e:\n",
    "        print(f\"A batch generated an exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Gl_X8aTNCkx"
   },
   "source": [
    "## Recreating Main table with Clustering and Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Step 3: Merging Temporary Table with Previous Main Table\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | Merging Temporary Table with Previous Main Table\")\n",
    "\n",
    "################################################################################\n",
    "\n",
    "label_colab = \"claim_preprocessing\" # Name of the colab; e.g. google_embeddings_vector_index\n",
    "label_step = \"creating_merged_table\"  # Descriptive name of what is happening, e.g. backlog_query\n",
    "label_pipeline = \"claim_etl_v0\" # Name of the BigQuery pipeline; e.g. patent_embeddings_v0\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "  labels = {\n",
    "    \"colab\": label_colab,\n",
    "    \"step\": label_step,\n",
    "    \"pipeline\": label_pipeline\n",
    "  }\n",
    ")\n",
    "\n",
    "append_output_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}_new_merged` AS (\n",
    "  SELECT\n",
    "    -- Add all the columns you want to keep from your tables, except for 'is_new'\n",
    "    * EXCEPT(is_new)\n",
    "  FROM (\n",
    "    -- Combine new and old records into a single source\n",
    "    SELECT\n",
    "      publication_number,\n",
    "      publication_date,\n",
    "      claim_id,\n",
    "      claim_num_str,\n",
    "      claim_num_int,\n",
    "      claim_text,\n",
    "      claim_type,\n",
    "      claim_list_parentclaim_ids,\n",
    "      claim_list_parentclaim_nums,\n",
    "      claim_list_parentclaim_texts,\n",
    "      claim_list_children_ids,\n",
    "      claim_immediate_parent_id,\n",
    "      claim_immediate_child_id,\n",
    "      claim_indep,\n",
    "      claim_indep_order,\n",
    "      processed_date,\n",
    "      CAST(processed_timestamp AS TIMESTAMP) AS processed_timestamp,\n",
    "      TRUE AS is_new -- Flag new records\n",
    "    FROM\n",
    "      `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}_new_{start_time}` t\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "      publication_number,\n",
    "      publication_date,\n",
    "      claim_id,\n",
    "      claim_num_str,\n",
    "      claim_num_int,\n",
    "      claim_text,\n",
    "      claim_type,\n",
    "      claim_list_parentclaim_ids,\n",
    "      claim_list_parentclaim_nums,\n",
    "      claim_list_parentclaim_texts,\n",
    "      claim_list_children_ids,\n",
    "      claim_immediate_parent_id,\n",
    "      claim_immediate_child_id,\n",
    "      claim_indep,\n",
    "      claim_indep_order,\n",
    "      processed_date,\n",
    "      CAST(processed_timestamp AS TIMESTAMP) AS processed_timestamp,\n",
    "      FALSE AS is_new -- Flag old records\n",
    "    FROM\n",
    "      `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}` t\n",
    "  )\n",
    "  -- Filter the results to get the newest row for each publication_number\n",
    "  QUALIFY ROW_NUMBER() OVER (PARTITION BY publication_number, claim_id ORDER BY is_new DESC, processed_date DESC) = 1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "append_output = bq_client.query(append_output_sql, job_config=job_config)\n",
    "try:\n",
    "  append_output_result = append_output.result()\n",
    "  if append_output.state == 'DONE':\n",
    "    print(\"Successfully Created Temporary new Main Table\")\n",
    "  else:\n",
    "    print(\"Please re-check the previous query\")\n",
    "except Exception as ex:\n",
    "  print(f\"Please re-check the previous query, error: {ex}\")\n",
    "  raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Step 4: Dropping Temporary Table\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | Dropping Temporary Table\")\n",
    "\n",
    "################################################################################\n",
    "\n",
    "label_colab = \"claim_preprocessing\" # Name of the colab; e.g. google_embeddings_vector_index\n",
    "label_step = \"dropping_temporary_table\"  # Descriptive name of what is happening, e.g. backlog_query\n",
    "label_pipeline = \"claim_etl_v0\" # Name of the BigQuery pipeline; e.g. patent_embeddings_v0\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "  labels = {\n",
    "    \"colab\": label_colab,\n",
    "    \"step\": label_step,\n",
    "    \"pipeline\": label_pipeline\n",
    "  }\n",
    ")\n",
    "\n",
    "drop_older_table = f\"DROP TABLE `{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}_new_{start_time}`\"\n",
    "drop_old_output = bq_client.query(drop_older_table, job_config=job_config)\n",
    "try:\n",
    "  drop_old_output_result = drop_old_output.result()\n",
    "  if drop_old_output.state == 'DONE':\n",
    "    print(\"Successfully Dropped Temp tables\")\n",
    "  else:\n",
    "    print(\"Please re-check the previous query\")\n",
    "except Exception as ex:\n",
    "  print(f\"Please re-check the previous query, error: {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Step 5: Dropping Old Main Table\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | Dropping Old Main Table\")\n",
    "\n",
    "################################################################################\n",
    "\n",
    "label_colab = \"claim_preprocessing\" # Name of the colab; e.g. google_embeddings_vector_index\n",
    "label_step = \"dropping_old_main_table\"  # Descriptive name of what is happening, e.g. backlog_query\n",
    "label_pipeline = \"claim_etl_v0\" # Name of the BigQuery pipeline; e.g. patent_embeddings_v0\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "  labels = {\n",
    "    \"colab\": label_colab,\n",
    "    \"step\": label_step,\n",
    "    \"pipeline\": label_pipeline\n",
    "  }\n",
    ")\n",
    "\n",
    "drop_older_table = f\"DROP TABLE `{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}`\"\n",
    "drop_old_output = bq_client.query(drop_older_table, job_config=job_config)\n",
    "try:\n",
    "  drop_old_output_result = drop_old_output.result()\n",
    "  if drop_old_output.state == 'DONE':\n",
    "    print(\"Successfully Dropped Old original table\")\n",
    "  else:\n",
    "    print(\"Please re-check the previous query\")\n",
    "except Exception as ex:\n",
    "  print(f\"Please re-check the previous query, error: {ex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#@title Step 6: Creating new Main Table\n",
    "print(f\"{get_timestamp(True)} | {get_uptime()} | Creating New Main Table\")\n",
    "\n",
    "################################################################################\n",
    "\n",
    "label_colab = \"claim_preprocessing\" # Name of the colab; e.g. google_embeddings_vector_index\n",
    "label_step = \"creating_new_main_table\"  # Descriptive name of what is happening, e.g. backlog_query\n",
    "label_pipeline = \"claim_etl_v0\" # Name of the BigQuery pipeline; e.g. patent_embeddings_v0\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "  labels = {\n",
    "    \"colab\": label_colab,\n",
    "    \"step\": label_step,\n",
    "    \"pipeline\": label_pipeline\n",
    "  }\n",
    ")\n",
    "\n",
    "append_output_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}`\n",
    "PARTITION BY processed_date\n",
    "CLUSTER BY claim_indep, claim_num_int\n",
    "AS (\n",
    "  SELECT\n",
    "    *\n",
    "  FROM `{GPATENTS_PROJECT}.{GPATENTS_DATASET_ID}.{OUTPUT_CLAIMS_TABLE_ID}_new_merged`\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "append_output = bq_client.query(append_output_sql, job_config=job_config)\n",
    "try:\n",
    "  append_output_result = append_output.result()\n",
    "  if append_output.state == 'DONE':\n",
    "    print(\"Successfully Created Output new Main Table\")\n",
    "  else:\n",
    "    print(\"Please re-check the previous query\")\n",
    "except Exception as ex:\n",
    "  print(f\"Please re-check the previous query, error: {ex}\")\n",
    "  raise ex"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
